{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rebecca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'avatar', 'rating', 'snippet', 'likes', 'date',\n",
      "       'iso_date', 'response'],\n",
      "      dtype='object')\n",
      "                                     id                       title  \\\n",
      "0  e5384431-56f9-43fa-a32a-53296afc7f66                    Seraphim   \n",
      "1  6a73081f-3490-47ba-89fa-83744cb20940                  TWOSTORE !   \n",
      "2  3a3d4c90-0b6e-45dc-b1e6-014659055bbf                         A G   \n",
      "3  99015538-1d26-4bd9-a02f-37bc2a361d1a                       Astra   \n",
      "4  541b3b4d-97f6-42e0-9c68-059a63e1e67f             Angela Williams   \n",
      "5  ad484b6a-1b9c-42ab-9cee-9df6e28f12d4                     Valerie   \n",
      "6  fb25cdf6-40d8-44e8-b1b9-d439ebd88565  Daniel “Chotara” Ricciardi   \n",
      "7  36e821d3-9441-4eaa-94a7-9c7b9b7463b5                         Amy   \n",
      "8  4b1e6dcb-d251-450e-9be6-358b4bb8e9d6                 Feitan Desy   \n",
      "9  27525772-1c0a-40e4-8321-4c5f0a0f7c64            Olivia Staringer   \n",
      "\n",
      "                                              avatar  rating  \\\n",
      "0  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
      "1  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
      "2  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
      "3  https://play-lh.googleusercontent.com/a-/ALV-U...     5.0   \n",
      "4  https://play-lh.googleusercontent.com/a-/ALV-U...     2.0   \n",
      "5  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
      "6  https://play-lh.googleusercontent.com/a-/ALV-U...     5.0   \n",
      "7  https://play-lh.googleusercontent.com/a-/ALV-U...     4.0   \n",
      "8  https://play-lh.googleusercontent.com/a-/ALV-U...     4.0   \n",
      "9  https://play-lh.googleusercontent.com/a-/ALV-U...     1.0   \n",
      "\n",
      "                                             snippet  likes  \\\n",
      "0  While I've reviewed this before, I decided to ...     88   \n",
      "1  Very fun but I wish there was more fighting in...     35   \n",
      "2  Fun game, I enjoy the story. There is tons to ...     11   \n",
      "3  This game is phenomenal. The art style and sce...     77   \n",
      "4  I love this game. However, it is incredibly la...     14   \n",
      "5  I used to be obsessed with this game but I hav...     99   \n",
      "6  Highly recommend. I have been playing the game...     65   \n",
      "7  Hello! I absolutely love this game! It has inc...     29   \n",
      "8  I've been playing this game since January 16th...     64   \n",
      "9  Uninteresting characters, uninteresting dialog...     14   \n",
      "\n",
      "                date              iso_date response  \n",
      "0   October 09, 2024  2024-10-09T00:08:20Z      NaN  \n",
      "1   October 12, 2024  2024-10-12T06:39:01Z      NaN  \n",
      "2   October 30, 2024  2024-10-30T18:01:37Z      NaN  \n",
      "3   October 18, 2024  2024-10-18T19:11:12Z      NaN  \n",
      "4   October 14, 2024  2024-10-14T03:04:43Z      NaN  \n",
      "5   October 13, 2024  2024-10-13T14:02:08Z      NaN  \n",
      "6   October 18, 2024  2024-10-18T15:20:11Z      NaN  \n",
      "7   October 29, 2024  2024-10-29T00:27:08Z      NaN  \n",
      "8   October 08, 2024  2024-10-08T23:13:09Z      NaN  \n",
      "9  November 11, 2024  2024-11-11T22:58:07Z      NaN  \n",
      "                                             snippet  \\\n",
      "0  While I've reviewed this before, I decided to ...   \n",
      "1  Very fun but I wish there was more fighting in...   \n",
      "2  Fun game, I enjoy the story. There is tons to ...   \n",
      "3  This game is phenomenal. The art style and sce...   \n",
      "4  I love this game. However, it is incredibly la...   \n",
      "\n",
      "                                     cleaned_snippet  \n",
      "0  ive review decid edit respons current opinion ...  \n",
      "1  fun wish fight quest bunch talk run around com...  \n",
      "2  fun game enjoy stori ton ton content clear you...  \n",
      "3  game phenomen art style sceneri stun yes game ...  \n",
      "4  love game howev incred laggi point unplay grap...  \n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'to', 'doing', \"weren't\", 'ours', 'between', 'once', 't', 'under', 'until', 'couldn', 'most', 'having', 'there', 'shan', 'theirs', 'which', 'now', 'my', 'more', 'him', \"hasn't\", 'very', 'all', 'further', 'because', 'will', 'these', \"needn't\", 's', 'is', 'has', 'be', 'he', 'won', 'during', 'above', 'at', 'have', 'each', 'with', 'can', 'hadn', 'up', 'those', 'do', 'ain', 'been', 'needn', \"you're\", 'if', 'through', 'ma', 'into', 'just', \"it's\", \"isn't\", 'her', 'other', 'no', 'did', 'below', 'wasn', 'such', \"hadn't\", 'them', 'does', 'll', \"wasn't\", \"that'll\", 'who', 'nor', 're', 'their', 'yourself', 'had', 've', 'of', 'again', 'after', 'haven', \"shouldn't\", 'mightn', \"couldn't\", 'itself', 'why', 'were', 'or', 'me', 'how', 'don', \"should've\", 'it', 'being', 'this', 'where', 'aren', 'for', 'few', 'too', 'a', 'out', 'but', 'an', 'same', 'should', 'in', 'its', \"didn't\", 'doesn', 'over', 'then', \"mightn't\", \"she's\", 'mustn', 'both', \"haven't\", 'your', 'about', 'she', \"wouldn't\", 'when', 'while', 'they', 'weren', 'down', 'not', 'hasn', 'here', 'we', 'that', \"you'll\", 'by', 'myself', 'whom', 'ourselves', 'themselves', 'the', 'what', \"doesn't\", 'yours', 'yourselves', 'before', 'didn', \"mustn't\", 'are', 'his', 'himself', 'so', 'from', \"you've\", 'm', 'against', 'i', 'was', 'am', 'shouldn', 'hers', 'some', \"shan't\", 'and', 'you', 'd', 'o', 'our', \"aren't\", \"won't\", 'isn', 'as', 'only', 'any', 'herself', 'wouldn', \"you'd\", \"don't\", 'off', 'y', 'than', 'on', 'own'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39mstop_words)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Fit dan transform data ulasan yang telah diproses menggunakan TF-IDF\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m X_tfidf \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_snippet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Gunakan 'cleaned_snippet' yang sudah diproses\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Ambil fitur kata kunci dari hasil TF-IDF\u001b[39;00m\n\u001b[1;32m     52\u001b[0m tfidf_features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n",
      "File \u001b[0;32m~/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2103\u001b[0m )\n\u001b[0;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/base.py:1382\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1378\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1379\u001b[0m )\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1382\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/base.py:436\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:98\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of TfidfVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'to', 'doing', \"weren't\", 'ours', 'between', 'once', 't', 'under', 'until', 'couldn', 'most', 'having', 'there', 'shan', 'theirs', 'which', 'now', 'my', 'more', 'him', \"hasn't\", 'very', 'all', 'further', 'because', 'will', 'these', \"needn't\", 's', 'is', 'has', 'be', 'he', 'won', 'during', 'above', 'at', 'have', 'each', 'with', 'can', 'hadn', 'up', 'those', 'do', 'ain', 'been', 'needn', \"you're\", 'if', 'through', 'ma', 'into', 'just', \"it's\", \"isn't\", 'her', 'other', 'no', 'did', 'below', 'wasn', 'such', \"hadn't\", 'them', 'does', 'll', \"wasn't\", \"that'll\", 'who', 'nor', 're', 'their', 'yourself', 'had', 've', 'of', 'again', 'after', 'haven', \"shouldn't\", 'mightn', \"couldn't\", 'itself', 'why', 'were', 'or', 'me', 'how', 'don', \"should've\", 'it', 'being', 'this', 'where', 'aren', 'for', 'few', 'too', 'a', 'out', 'but', 'an', 'same', 'should', 'in', 'its', \"didn't\", 'doesn', 'over', 'then', \"mightn't\", \"she's\", 'mustn', 'both', \"haven't\", 'your', 'about', 'she', \"wouldn't\", 'when', 'while', 'they', 'weren', 'down', 'not', 'hasn', 'here', 'we', 'that', \"you'll\", 'by', 'myself', 'whom', 'ourselves', 'themselves', 'the', 'what', \"doesn't\", 'yours', 'yourselves', 'before', 'didn', \"mustn't\", 'are', 'his', 'himself', 'so', 'from', \"you've\", 'm', 'against', 'i', 'was', 'am', 'shouldn', 'hers', 'some', \"shan't\", 'and', 'you', 'd', 'o', 'our', \"aren't\", \"won't\", 'isn', 'as', 'only', 'any', 'herself', 'wouldn', \"you'd\", \"don't\", 'off', 'y', 'than', 'on', 'own'} instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Pastikan ini diimpor\n",
    "\n",
    "# Unduh stopwords jika belum ada\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Baca file CSV\n",
    "df = pd.read_csv('data/google-play-rev-gen-2.csv')\n",
    "\n",
    "# Cek kolom yang ada dalam dataset\n",
    "print(df.columns)\n",
    "\n",
    "# Tampilkan beberapa baris pertama data untuk memastikan data ter-load dengan benar\n",
    "print(df.head(10))\n",
    "\n",
    "# Inisialisasi stopwords dan stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Menghapus karakter khusus, angka, dan tanda baca\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenisasi: Pisahkan teks menjadi list kata\n",
    "    tokens = text.lower().split()\n",
    "    \n",
    "    # Menghapus stopwords dan stemming\n",
    "    processed_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Gabungkan kembali tokens yang telah diproses menjadi teks yang bersih\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Pastikan nama kolom 'snippet' sesuai dengan data yang ada\n",
    "# Terapkan preprocessing pada kolom 'snippet' yang berisi ulasan\n",
    "df['cleaned_snippet'] = df['snippet'].apply(preprocess_text)\n",
    "\n",
    "# Tampilkan contoh hasil setelah preprocessing\n",
    "print(df[['snippet', 'cleaned_snippet']].head())\n",
    "\n",
    "# Inisialisasi TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit dan transform data ulasan yang telah diproses menggunakan TF-IDF\n",
    "X_tfidf = vectorizer.fit_transform(df['cleaned_snippet'])  # Gunakan 'cleaned_snippet' yang sudah diproses\n",
    "\n",
    "# Ambil fitur kata kunci dari hasil TF-IDF\n",
    "tfidf_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Konversi hasil TF-IDF menjadi DataFrame agar lebih mudah dibaca\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_features)\n",
    "\n",
    "# Menampilkan hasil TF-IDF\n",
    "print(tfidf_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
