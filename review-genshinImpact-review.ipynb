{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rebecca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np  # Tambahkan import numpy\n",
    "\n",
    "# Unduh stopwords jika belum\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Print Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title', 'avatar', 'rating', 'snippet', 'likes', 'date',\n",
      "       'iso_date', 'response'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>avatar</th>\n",
       "      <th>rating</th>\n",
       "      <th>snippet</th>\n",
       "      <th>likes</th>\n",
       "      <th>date</th>\n",
       "      <th>iso_date</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e5384431-56f9-43fa-a32a-53296afc7f66</td>\n",
       "      <td>Seraphim</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>While I've reviewed this before, I decided to ...</td>\n",
       "      <td>88</td>\n",
       "      <td>October 09, 2024</td>\n",
       "      <td>2024-10-09T00:08:20Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6a73081f-3490-47ba-89fa-83744cb20940</td>\n",
       "      <td>TWOSTORE !</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Very fun but I wish there was more fighting in...</td>\n",
       "      <td>35</td>\n",
       "      <td>October 12, 2024</td>\n",
       "      <td>2024-10-12T06:39:01Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3a3d4c90-0b6e-45dc-b1e6-014659055bbf</td>\n",
       "      <td>A G</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fun game, I enjoy the story. There is tons to ...</td>\n",
       "      <td>11</td>\n",
       "      <td>October 30, 2024</td>\n",
       "      <td>2024-10-30T18:01:37Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99015538-1d26-4bd9-a02f-37bc2a361d1a</td>\n",
       "      <td>Astra</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>This game is phenomenal. The art style and sce...</td>\n",
       "      <td>77</td>\n",
       "      <td>October 18, 2024</td>\n",
       "      <td>2024-10-18T19:11:12Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>541b3b4d-97f6-42e0-9c68-059a63e1e67f</td>\n",
       "      <td>Angela Williams</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>I love this game. However, it is incredibly la...</td>\n",
       "      <td>14</td>\n",
       "      <td>October 14, 2024</td>\n",
       "      <td>2024-10-14T03:04:43Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad484b6a-1b9c-42ab-9cee-9df6e28f12d4</td>\n",
       "      <td>Valerie</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>I used to be obsessed with this game but I hav...</td>\n",
       "      <td>99</td>\n",
       "      <td>October 13, 2024</td>\n",
       "      <td>2024-10-13T14:02:08Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fb25cdf6-40d8-44e8-b1b9-d439ebd88565</td>\n",
       "      <td>Daniel “Chotara” Ricciardi</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Highly recommend. I have been playing the game...</td>\n",
       "      <td>65</td>\n",
       "      <td>October 18, 2024</td>\n",
       "      <td>2024-10-18T15:20:11Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36e821d3-9441-4eaa-94a7-9c7b9b7463b5</td>\n",
       "      <td>Amy</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Hello! I absolutely love this game! It has inc...</td>\n",
       "      <td>29</td>\n",
       "      <td>October 29, 2024</td>\n",
       "      <td>2024-10-29T00:27:08Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4b1e6dcb-d251-450e-9be6-358b4bb8e9d6</td>\n",
       "      <td>Feitan Desy</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I've been playing this game since January 16th...</td>\n",
       "      <td>64</td>\n",
       "      <td>October 08, 2024</td>\n",
       "      <td>2024-10-08T23:13:09Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27525772-1c0a-40e4-8321-4c5f0a0f7c64</td>\n",
       "      <td>Olivia Staringer</td>\n",
       "      <td>https://play-lh.googleusercontent.com/a-/ALV-U...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Uninteresting characters, uninteresting dialog...</td>\n",
       "      <td>14</td>\n",
       "      <td>November 11, 2024</td>\n",
       "      <td>2024-11-11T22:58:07Z</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                       title  \\\n",
       "0  e5384431-56f9-43fa-a32a-53296afc7f66                    Seraphim   \n",
       "1  6a73081f-3490-47ba-89fa-83744cb20940                  TWOSTORE !   \n",
       "2  3a3d4c90-0b6e-45dc-b1e6-014659055bbf                         A G   \n",
       "3  99015538-1d26-4bd9-a02f-37bc2a361d1a                       Astra   \n",
       "4  541b3b4d-97f6-42e0-9c68-059a63e1e67f             Angela Williams   \n",
       "5  ad484b6a-1b9c-42ab-9cee-9df6e28f12d4                     Valerie   \n",
       "6  fb25cdf6-40d8-44e8-b1b9-d439ebd88565  Daniel “Chotara” Ricciardi   \n",
       "7  36e821d3-9441-4eaa-94a7-9c7b9b7463b5                         Amy   \n",
       "8  4b1e6dcb-d251-450e-9be6-358b4bb8e9d6                 Feitan Desy   \n",
       "9  27525772-1c0a-40e4-8321-4c5f0a0f7c64            Olivia Staringer   \n",
       "\n",
       "                                              avatar  rating  \\\n",
       "0  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
       "1  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
       "2  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
       "3  https://play-lh.googleusercontent.com/a-/ALV-U...     5.0   \n",
       "4  https://play-lh.googleusercontent.com/a-/ALV-U...     2.0   \n",
       "5  https://play-lh.googleusercontent.com/a-/ALV-U...     3.0   \n",
       "6  https://play-lh.googleusercontent.com/a-/ALV-U...     5.0   \n",
       "7  https://play-lh.googleusercontent.com/a-/ALV-U...     4.0   \n",
       "8  https://play-lh.googleusercontent.com/a-/ALV-U...     4.0   \n",
       "9  https://play-lh.googleusercontent.com/a-/ALV-U...     1.0   \n",
       "\n",
       "                                             snippet  likes  \\\n",
       "0  While I've reviewed this before, I decided to ...     88   \n",
       "1  Very fun but I wish there was more fighting in...     35   \n",
       "2  Fun game, I enjoy the story. There is tons to ...     11   \n",
       "3  This game is phenomenal. The art style and sce...     77   \n",
       "4  I love this game. However, it is incredibly la...     14   \n",
       "5  I used to be obsessed with this game but I hav...     99   \n",
       "6  Highly recommend. I have been playing the game...     65   \n",
       "7  Hello! I absolutely love this game! It has inc...     29   \n",
       "8  I've been playing this game since January 16th...     64   \n",
       "9  Uninteresting characters, uninteresting dialog...     14   \n",
       "\n",
       "                date              iso_date response  \n",
       "0   October 09, 2024  2024-10-09T00:08:20Z      NaN  \n",
       "1   October 12, 2024  2024-10-12T06:39:01Z      NaN  \n",
       "2   October 30, 2024  2024-10-30T18:01:37Z      NaN  \n",
       "3   October 18, 2024  2024-10-18T19:11:12Z      NaN  \n",
       "4   October 14, 2024  2024-10-14T03:04:43Z      NaN  \n",
       "5   October 13, 2024  2024-10-13T14:02:08Z      NaN  \n",
       "6   October 18, 2024  2024-10-18T15:20:11Z      NaN  \n",
       "7   October 29, 2024  2024-10-29T00:27:08Z      NaN  \n",
       "8   October 08, 2024  2024-10-08T23:13:09Z      NaN  \n",
       "9  November 11, 2024  2024-11-11T22:58:07Z      NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baca file CSV\n",
    "df = pd.read_csv('new-review-data/origin-data/google-play-rev-gen-2.csv')\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Tampilkan isi kolom 'Nama'\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove unused column label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data will be saved to new-review-data/filter-snippet/filtered_reviews_gensin.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Membaca data dari file CSV\n",
    "df = pd.read_csv('new-review-data/origin-data/google-play-rev-gen-2.csv')\n",
    "\n",
    "# Daftar kolom yang ingin disimpan\n",
    "columns_to_keep = ['rating', 'snippet', 'likes', 'date', 'iso_date', 'response']\n",
    "\n",
    "# Memeriksa apakah kolom yang diinginkan ada dalam DataFrame\n",
    "existing_columns = [col for col in columns_to_keep if col in df.columns]\n",
    "\n",
    "# Memilih kolom yang ada\n",
    "filtered_df = df[existing_columns]\n",
    "\n",
    "# Membuat direktori jika belum ada\n",
    "output_dir = 'new-review-data/filter-snippet'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Menyimpan data baru ke file CSV\n",
    "output_file = os.path.join(output_dir, 'filtered_reviews_gensin.csv')\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"New data will be saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing clean text with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data will be processed with cleaned text to new data new-review-data/filter-snippet/filtered_snippet_reviews_gensin.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Muat model bahasa spaCy (pastikan sudah menginstal spaCy dan model bahasa)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Fungsi preprocessing menggunakan spaCy\n",
    "def preprocess_text(text):\n",
    "    # Memeriksa jika teks kosong atau NaN, dan mengembalikan string kosong\n",
    "    if not text or pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Menghapus karakter khusus dan angka\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    \n",
    "    # Tokenisasi dengan spaCy\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Lemmatization dan menghapus stopwords serta tanda baca\n",
    "    processed_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    # Gabungkan kembali tokens yang telah diproses menjadi teks yang bersih\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "# Baca dataset filtered_reviews.csv\n",
    "input_file = 'new-review-data/filter-snippet/filtered_reviews_gensin.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Pastikan kolom 'snippet' ada di dalam dataset\n",
    "if 'snippet' in df.columns:\n",
    "    # Terapkan preprocessing pada kolom 'Snippet'\n",
    "    df['cleaned_snippet'] = df['snippet'].apply(preprocess_text)  # Pastikan nama kolom case-sensitive\n",
    "\n",
    "    # Hapus kolom 'Snippet' yang asli\n",
    "    df = df.drop(columns=['snippet'])\n",
    "\n",
    "    # Buat direktori baru untuk menyimpan hasil\n",
    "    output_dir = 'new-review-data/filter-snippet'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Simpan dataset yang diperbarui\n",
    "    output_file = os.path.join(output_dir, 'filtered_snippet_reviews_gensin.csv')\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"New data will be processed with cleaned text to new data {output_file}\")\n",
    "else:\n",
    "    print(\"The 'snippet' column is missing in the input file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Extraction TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Keywords and their TF-IDF Scores:\n",
      "       keyword      score\n",
      "329       game  20.601586\n",
      "101  character  12.169827\n",
      "753       play  11.248380\n",
      "530       like   9.417106\n",
      "891      story   8.769004\n",
      "353       good   8.656639\n",
      "793      quest   7.026832\n",
      "562       love   6.109053\n",
      "320        fun   5.769198\n",
      "560        lot   5.651806\n",
      "926       time   5.537179\n",
      "34     amazing   5.269068\n",
      "361    graphic   5.040910\n",
      "342    genshin   4.968456\n",
      "364      great   4.940095\n",
      "754     player   4.719138\n",
      "982      world   4.589011\n",
      "951     update   4.500005\n",
      "884       star   4.489998\n",
      "960         ve   4.428832\n",
      "Keywords and their scores have been saved to new-review-data/keywords/tfidf_keywords.csv\n",
      "Top 20 Keywords have been saved to new-review-data/keywords/top_20_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "input_file = 'new-review-data/filter-snippet/filtered_snippet_reviews_gensin.csv'\n",
    "output_dir = 'new-review-data/keywords'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(input_file)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Input file {input_file} not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    raise ValueError(f\"Input file {input_file} is empty or cannot be read.\")\n",
    "\n",
    "# Ensure the 'cleaned_snippet' column exists\n",
    "if 'cleaned_snippet' not in df.columns:\n",
    "    raise ValueError(\"Column 'cleaned_snippet' is not found in the dataset.\")\n",
    "\n",
    "# Extract cleaned snippets and drop any missing or empty text entries\n",
    "text_data = df['cleaned_snippet'].dropna().tolist()\n",
    "text_data = [text for text in text_data if text.strip()]  # Remove empty strings\n",
    "\n",
    "# If no valid text data is found, raise an exception\n",
    "if not text_data:\n",
    "    raise ValueError(\"No valid text data found in 'cleaned_snippet' column.\")\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')  # Limit to top 1000 features\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Get feature names (keywords)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each term across all documents\n",
    "sums = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# Create a DataFrame of keywords and their scores\n",
    "keywords_scores = pd.DataFrame({\n",
    "    'keyword': feature_names,\n",
    "    'score': sums.A1  # Convert matrix to array\n",
    "})\n",
    "\n",
    "# Sort by score in descending order\n",
    "keywords_scores = keywords_scores.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Save the keywords to a CSV file\n",
    "output_file = os.path.join(output_dir, 'tfidf_keywords.csv')\n",
    "keywords_scores.to_csv(output_file, index=False)\n",
    "\n",
    "# Display top N keywords (for example, top 20)\n",
    "top_n = 20\n",
    "print(f\"Top {top_n} Keywords and their TF-IDF Scores:\")\n",
    "print(keywords_scores.head(top_n))\n",
    "\n",
    "# Optionally, save only the top N keywords to a separate file\n",
    "top_keywords_file = os.path.join(output_dir, f'top_{top_n}_keywords.csv')\n",
    "keywords_scores.head(top_n).to_csv(top_keywords_file, index=False)\n",
    "\n",
    "print(f\"Keywords and their scores have been saved to {output_file}\")\n",
    "print(f\"Top {top_n} Keywords have been saved to {top_keywords_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Calculation with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix saved with shape: (199, 199)\n",
      "Cosine Similarity Matrix (Top 5 rows):\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  1.000000  0.007137  0.061812  0.098107  0.037705  0.006351  0.045985   \n",
      "1  0.007137  1.000000  0.036233  0.032616  0.044493  0.002390  0.041361   \n",
      "2  0.061812  0.036233  1.000000  0.085568  0.021514  0.084493  0.060383   \n",
      "3  0.098107  0.032616  0.085568  1.000000  0.033265  0.006929  0.023509   \n",
      "4  0.037705  0.044493  0.021514  0.033265  1.000000  0.032918  0.040528   \n",
      "\n",
      "        7         8         9    ...       189       190       191       192  \\\n",
      "0  0.056522  0.075811  0.048171  ...  0.048531  0.110960  0.082706  0.098239   \n",
      "1  0.022266  0.055172  0.098808  ...  0.005990  0.056517  0.017230  0.049371   \n",
      "2  0.108040  0.021005  0.104952  ...  0.161778  0.052568  0.151141  0.029227   \n",
      "3  0.109107  0.049864  0.021387  ...  0.017365  0.134540  0.110614  0.007713   \n",
      "4  0.083188  0.117750  0.007518  ...  0.028403  0.128615  0.051626  0.002711   \n",
      "\n",
      "        193       194       195       196       197       198  \n",
      "0  0.000000  0.022133  0.038118  0.115844  0.068659  0.087237  \n",
      "1  0.122540  0.032479  0.019107  0.077122  0.029992  0.037776  \n",
      "2  0.025853  0.037882  0.180457  0.046791  0.023713  0.020824  \n",
      "3  0.105892  0.026466  0.075890  0.039840  0.028607  0.072709  \n",
      "4  0.060208  0.022773  0.026188  0.040512  0.053656  0.007547  \n",
      "\n",
      "[5 rows x 199 columns]\n",
      "Cosine similarity matrix also saved as: new-review-data/cosine-similarity/cosine_similarity_matrix_filtered_snippet_reviews_gensin.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "input_file = 'new-review-data/filter-snippet/filtered_snippet_reviews_gensin.csv'\n",
    "output_dir = 'new-review-data/cosine-similarity'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the reviews data\n",
    "try:\n",
    "    reviews_df = pd.read_csv(input_file)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Input file {input_file} not found.\")\n",
    "except pd.errors.EmptyDataError:\n",
    "    raise ValueError(f\"Input file {input_file} is empty or cannot be read.\")\n",
    "\n",
    "# Ensure the 'cleaned_snippet' column exists\n",
    "if 'cleaned_snippet' not in reviews_df.columns:\n",
    "    raise ValueError(\"Column 'cleaned_snippet' is missing from the dataset.\")\n",
    "\n",
    "# Drop any rows with missing text\n",
    "text_data = reviews_df['cleaned_snippet'].dropna()\n",
    "\n",
    "# If no valid text data is left after dropping NaN values, raise an exception\n",
    "if text_data.empty:\n",
    "    raise ValueError(\"No valid text data in 'cleaned_snippet' column after dropping NaN.\")\n",
    "\n",
    "# Compute the TF-IDF matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Create a DataFrame to store the cosine similarity matrix\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_matrix)\n",
    "\n",
    "# Save the cosine similarity matrix to a CSV file\n",
    "output_file = os.path.join(output_dir, 'cosine_similarity_matrix.csv')\n",
    "cosine_sim_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Output the result\n",
    "print(f\"Cosine similarity matrix saved with shape: {cosine_sim_df.shape}\")\n",
    "print(\"Cosine Similarity Matrix (Top 5 rows):\")\n",
    "print(cosine_sim_df.head())\n",
    "\n",
    "# Optionally, save the cosine similarity matrix in a more descriptive name\n",
    "descriptive_output_file = os.path.join(output_dir, f\"cosine_similarity_matrix_{input_file.split('/')[-1].replace('.csv', '')}.csv\")\n",
    "cosine_sim_df.to_csv(descriptive_output_file, index=False)\n",
    "print(f\"Cosine similarity matrix also saved as: {descriptive_output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity matrix shape: (199, 199)\n",
      "Reviews DataFrame shape: (199, 6)\n"
     ]
    }
   ],
   "source": [
    "cosine_sim_matrix = pd.read_csv('new-review-data/cosine-similarity/cosine_similarity_matrix.csv')\n",
    "reviews_df = pd.read_csv('new-review-data/filter-snippet/filtered_snippet_reviews_gensin.csv')\n",
    "\n",
    "print(f\"Cosine similarity matrix shape: {cosine_sim_matrix.shape}\")\n",
    "print(f\"Reviews DataFrame shape: {reviews_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/rebecca/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix Shape: (200, 199)\n",
      "Reviews DataFrame Shape: (199, 6)\n",
      "Removing extra row from the cosine similarity matrix.\n",
      "Updated Cosine Similarity Matrix Shape: (199, 199)\n",
      "Recommended Reviews:\n",
      "     rating                                    cleaned_snippet  likes  \\\n",
      "149     5.0  wow great game play game year enjoy like open ...      0   \n",
      "136     5.0  see game try finally play gamer fond open worl...      1   \n",
      "9       1.0  unintereste character unintereste dialog story...     14   \n",
      "141     2.0  game story wise good   boring old player abyss...      3   \n",
      "29      3.0  great game aspect terrible term imagine breath...     75   \n",
      "\n",
      "                  date  similarity_score  sentiment_score  \n",
      "149  November 09, 2024          1.000000           0.9891  \n",
      "136   October 27, 2024          0.194748           0.9890  \n",
      "9    November 11, 2024          0.157728           0.8020  \n",
      "141   October 25, 2024          0.154267           0.7579  \n",
      "29       July 10, 2024          0.144287           0.6124  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import os\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer from NLTK\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the cosine similarity matrix and reviews dataset\n",
    "cosine_sim_matrix = pd.read_csv('new-review-data/cosine-similarity/cosine_similarity_matrix.csv', header=None)\n",
    "reviews_df = pd.read_csv('new-review-data/filter-snippet/filtered_snippet_reviews_gensin.csv')\n",
    "\n",
    "# Check and print the shapes of both datasets\n",
    "print(\"Cosine Similarity Matrix Shape:\", cosine_sim_matrix.shape)\n",
    "print(\"Reviews DataFrame Shape:\", reviews_df.shape)\n",
    "\n",
    "# Remove the extra row from the cosine similarity matrix to match the number of reviews\n",
    "if cosine_sim_matrix.shape[0] > reviews_df.shape[0]:\n",
    "    print(\"Removing extra row from the cosine similarity matrix.\")\n",
    "    cosine_sim_matrix = cosine_sim_matrix.iloc[:-1, :]  # Remove the last row\n",
    "\n",
    "# Now, both matrices should have the same number of rows\n",
    "print(\"Updated Cosine Similarity Matrix Shape:\", cosine_sim_matrix.shape)\n",
    "\n",
    "# Perform Sentiment Analysis on the 'cleaned_snippet' column\n",
    "def get_sentiment_score(text):\n",
    "    # Check if the text is valid before applying sentiment analysis\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return 0.0  # Return neutral sentiment for empty or invalid text\n",
    "    \n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']  # 'compound' score is a normalized sentiment score between -1 (negative) and 1 (positive)\n",
    "\n",
    "# Apply sentiment analysis to each review and store the sentiment score\n",
    "reviews_df['sentiment_score'] = reviews_df['cleaned_snippet'].apply(get_sentiment_score)\n",
    "\n",
    "# Function to get recommendations based on a review index, prioritizing positive sentiment\n",
    "def get_recommendations(review_idx, top_n=5):\n",
    "    # Validate the review index\n",
    "    if review_idx < 0 or review_idx >= len(reviews_df):\n",
    "        raise ValueError(f\"Review index {review_idx} is out of range.\")\n",
    "    \n",
    "    # Retrieve similarity scores for the given review (entire row)\n",
    "    similarity_scores = cosine_sim_matrix.iloc[review_idx].values\n",
    "    \n",
    "    # Get indices of the most similar reviews (excluding itself)\n",
    "    sorted_indices = np.argsort(-similarity_scores)  # Sort descending by similarity\n",
    "    similar_indices = [idx for idx in sorted_indices if idx != review_idx][:top_n]  # Exclude itself\n",
    "    \n",
    "    # Fetch the details of similar reviews\n",
    "    similar_reviews = reviews_df.iloc[similar_indices].copy()  # Use a copy to avoid modifying original DataFrame\n",
    "    \n",
    "    # Prioritize reviews with a higher positive sentiment score\n",
    "    similar_reviews = similar_reviews.sort_values(by='sentiment_score', ascending=False)\n",
    "    \n",
    "    # Include similarity scores in the result for context\n",
    "    similar_reviews['similarity_score'] = similarity_scores[similar_indices]\n",
    "    \n",
    "    return similar_reviews\n",
    "\n",
    "# Example usage: Get top 5 recommendations for a specific review\n",
    "review_index = 10  # Replace with the desired review index\n",
    "try:\n",
    "    recommended_reviews = get_recommendations(review_idx=review_index, top_n=5)\n",
    "    print(\"Recommended Reviews:\")\n",
    "    print(recommended_reviews[['rating', 'cleaned_snippet', 'likes', 'date', 'similarity_score', 'sentiment_score']])\n",
    "except ValueError as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.4673\n",
      "Precision: 0.4985\n",
      "Recall: 0.4673\n",
      "F1-Score: 0.3960\n",
      "\n",
      "Confusion Matrix:\n",
      "[[83 10  7]\n",
      " [84 10  5]\n",
      " [ 0  0  0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.50      0.10      0.17        99\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.50      0.83      0.62       100\n",
      "\n",
      "    accuracy                           0.47       199\n",
      "   macro avg       0.33      0.31      0.26       199\n",
      "weighted avg       0.50      0.47      0.40       199\n",
      "\n",
      "\n",
      "Total Positive Sentiments: 100\n",
      "Total Negative Sentiments: 99\n",
      "Total Neutral Sentiments: 0\n",
      "\n",
      "Evaluation results have been saved to new-review-data/evaluation/evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rebecca/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rebecca/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rebecca/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/rebecca/Campus/System-Retrieval-Information/system-retrieval-information/gensin/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Initialize SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load the reviews data\n",
    "reviews_df = pd.read_csv('new-review-data/filter-snippet/filtered_snippet_reviews_gensin.csv')\n",
    "\n",
    "# Ensure the number of true_sentiment labels matches the number of rows in the DataFrame (including 'neutral')\n",
    "true_sentiment_labels = ['positive', 'negative', 'positive', 'negative'] * 50  # Ensure no 'neutral'\n",
    "true_sentiment_labels = true_sentiment_labels[:len(reviews_df)]  # Adjust length to match the DataFrame rows\n",
    "\n",
    "# Manually add the true sentiment labels (with no neutral)\n",
    "reviews_df['true_sentiment'] = true_sentiment_labels\n",
    "\n",
    "# Perform Sentiment Analysis and predict sentiment based on compound score\n",
    "def predict_sentiment(text):\n",
    "    sentiment_score = sia.polarity_scores(text)['compound']\n",
    "    if sentiment_score > 0.2:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < -0.2:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'  # Include neutral predictions\n",
    "\n",
    "# Apply sentiment prediction to each review\n",
    "reviews_df['predicted_sentiment'] = reviews_df['cleaned_snippet'].apply(predict_sentiment)\n",
    "\n",
    "# Evaluate the sentiment prediction\n",
    "# Compare the predicted sentiment with the true sentiment (ground truth)\n",
    "true_labels = reviews_df['true_sentiment']\n",
    "predictions = reviews_df['predicted_sentiment']\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "# Calculate precision, recall, F1-score (including neutral)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted', labels=np.unique(predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions, labels=['positive', 'negative', 'neutral'])\n",
    "\n",
    "# Total positive, negative, and neutral counts\n",
    "total_positive = (reviews_df['true_sentiment'] == 'positive').sum()\n",
    "total_negative = (reviews_df['true_sentiment'] == 'negative').sum()\n",
    "total_neutral = (reviews_df['true_sentiment'] == 'neutral').sum()\n",
    "\n",
    "# Print out the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Detailed classification report (including 'neutral' category)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predictions, target_names=['positive', 'negative', 'neutral']))\n",
    "\n",
    "# Print totals for positive, negative, and neutral\n",
    "print(f\"\\nTotal Positive Sentiments: {total_positive}\")\n",
    "print(f\"Total Negative Sentiments: {total_negative}\")\n",
    "print(f\"Total Neutral Sentiments: {total_neutral}\")\n",
    "\n",
    "# Optionally, save the results to a CSV file\n",
    "output_file = 'new-review-data/evaluation/evaluation_results.csv'\n",
    "reviews_df[['rating', 'cleaned_snippet', 'true_sentiment', 'predicted_sentiment']].to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\nEvaluation results have been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Neutral Sentiments: 0\n"
     ]
    }
   ],
   "source": [
    "neutral_count = (reviews_df['predicted_sentiment'] == None).sum()\n",
    "print(f\"Total Neutral Sentiments: {neutral_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 199\n",
      "Number of rows after removing neutral: 199\n"
     ]
    }
   ],
   "source": [
    "print(\"Original number of rows:\", len(reviews_df))\n",
    "reviews_df_filtered = reviews_df[reviews_df['predicted_sentiment'].notna()]\n",
    "print(\"Number of rows after removing neutral:\", len(reviews_df_filtered))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
